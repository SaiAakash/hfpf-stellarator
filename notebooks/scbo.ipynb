{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scalable Constrained Bayesian Optimization (SCBO) of Plasma boundaries for Stellarator Design**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all necessary libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saiaakashramesh/Library/Caches/pypoetry/virtualenvs/hf-stellarator-NEffnWGp-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gpytorch\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.models.transforms.input import Normalize\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "\n",
    "# Constrained Max Posterior Sampling s a new sampling class, similar to MaxPosteriorSampling,\n",
    "# which implements the constrained version of Thompson Sampling described in [1].\n",
    "from botorch.generation.sampling import ConstrainedMaxPosteriorSampling\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double\n",
    "tkwargs = {\"device\": device, \"dtype\": dtype}\n",
    "\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add normalization transform on the fourier coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_minmax(r_cos_data, z_sin_data, margin=0.05):\n",
    "    \"\"\"Min-Max scaling with physical margins.\"\"\"\n",
    "    # Compute bounds with margins for exploration\n",
    "    r_cos_min = r_cos_data.min(dim=0)[0]\n",
    "    r_cos_max = r_cos_data.max(dim=0)[0]\n",
    "    r_cos_range = r_cos_max - r_cos_min\n",
    "\n",
    "    z_sin_min = z_sin_data.min(dim=0)[0]\n",
    "    z_sin_max = z_sin_data.max(dim=0)[0]\n",
    "    z_sin_range = z_sin_max - z_sin_min\n",
    "\n",
    "    # Store bounds for normalization\n",
    "    bounds = torch.vstack(\n",
    "        [\n",
    "            torch.cat(\n",
    "                [\n",
    "                    r_cos_min - margin * r_cos_range,\n",
    "                    z_sin_min - margin * z_sin_range,\n",
    "                ]\n",
    "            ),\n",
    "            torch.cat(\n",
    "                [\n",
    "                    r_cos_max + margin * r_cos_range,\n",
    "                    z_sin_max + margin * z_sin_range,\n",
    "                ]\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def transform(x, bounds):\n",
    "    \"\"\"Normalize data to [0,1]^d.\"\"\"\n",
    "    return normalize(x, bounds=bounds)\n",
    "\n",
    "\n",
    "def untransform(x, bounds):\n",
    "    \"\"\"Unnormalize data from [0,1]^d to original space.\"\"\"\n",
    "    return unnormalize(x, bounds=bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data and fit the normalization transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_coefficients = 45\n",
    "\n",
    "# Load data and filter out NaN values\n",
    "data_tensor = torch.load(\"../data/batch_1.pt\").to(dtype=dtype)\n",
    "mask = torch.isnan(data_tensor).any(dim=1)\n",
    "filtered_tensor = data_tensor[~mask]\n",
    "\n",
    "r_cos_data = filtered_tensor[:, :n_coefficients]\n",
    "z_sin_data = filtered_tensor[:, n_coefficients : n_coefficients * 2]\n",
    "\n",
    "# Fit bounds for normalization\n",
    "bounds = fit_minmax(r_cos_data, z_sin_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the evaluator class to evaluate objectives and constraints using the forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constellaration.forward_model import forward_model\n",
    "from constellaration.geometry.surface_rz_fourier import SurfaceRZFourier\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Class for evaluating a plasma boundary with stellarator symmetry constraints.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bounds):\n",
    "        self.n_poloidal_modes = 5\n",
    "        self.n_toroidal_modes = 9\n",
    "        self.n_coeffecients = self.n_poloidal_modes * self.n_toroidal_modes\n",
    "        self.aspect_ratio_upper_bound = 4.0\n",
    "        self.average_triangularity_upper_bound = -0.5\n",
    "        self.edge_rotational_transform_over_n_field_periods_lower_bound = 0.3\n",
    "        self.bounds = bounds\n",
    "        self._cache = {}\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        \"\"\"Evaluate the forward model once and cache the result.\"\"\"\n",
    "        x_tuple = tuple(x)  # Ensure x is hashable for caching\n",
    "        if x_tuple not in self._cache.keys():\n",
    "            # Unnormalize before doing anything with the data\n",
    "            # x = untransform(x, self.bounds)\n",
    "\n",
    "            # Apply constraints to enforce stellarator symmetry\n",
    "            # x = self.enforce_constraints(x)\n",
    "\n",
    "            # Retrieve Fourier series coefficients from the unnormalized input and reshape according to the number of angular modes\n",
    "            r_cos = x[: self.n_coeffecients].reshape(\n",
    "                self.n_poloidal_modes, self.n_toroidal_modes\n",
    "            )\n",
    "            z_sin = x[self.n_coeffecients : 2 * self.n_coeffecients].reshape(\n",
    "                self.n_poloidal_modes, self.n_toroidal_modes\n",
    "            )\n",
    "\n",
    "            # Instantiate the boundary surface with the Fourier coefficients\n",
    "            boundary_surface = SurfaceRZFourier(\n",
    "                r_cos=r_cos.numpy(), z_sin=z_sin.numpy()\n",
    "            )\n",
    "\n",
    "            # Evaluate the forward model with the boundary surface and retrieve results\n",
    "            metrics, _ = forward_model(boundary=boundary_surface)\n",
    "            self._cache[x_tuple] = {\n",
    "                \"max_elongation\": metrics.max_elongation,\n",
    "                \"aspect_ratio\": metrics.aspect_ratio,\n",
    "                \"average_triangularity\": metrics.average_triangularity,\n",
    "                \"edge_rotational_transform_over_n_field_periods\": metrics.edge_rotational_transform_over_n_field_periods,\n",
    "            }\n",
    "\n",
    "        return self._cache[x_tuple]\n",
    "\n",
    "    def get_objective(self, x):\n",
    "        results = self.evaluate(x)\n",
    "        return results[tuple(x)][\"max_elongation\"]  # Objective based on max_elongation\n",
    "\n",
    "    def aspect_ratio_constraint(self, x):\n",
    "        # Requires get_objective for the same x to be run prior to computation of constraints\n",
    "        constraint_value = (\n",
    "            self._cache[tuple(x)][\"aspect_ratio\"] - self.aspect_ratio_upper_bound\n",
    "        )\n",
    "        return constraint_value\n",
    "\n",
    "    def average_triangularity_constraint(self, x):\n",
    "        # Requires get_objective for the same x to be run prior to computation of constraints\n",
    "        constraint_value = (\n",
    "            self._cache[tuple(x)][\"average_triangularity\"]\n",
    "            - self.average_triangularity_upper_bound\n",
    "        )\n",
    "        return constraint_value\n",
    "\n",
    "    def edge_rotational_transform_over_n_field_periods_constraint(self, x):\n",
    "        # Requires get_objective for the same x to be run prior to computation of constraints\n",
    "        constraint_value = (\n",
    "            self.edge_rotational_transform_over_n_field_periods_lower_bound\n",
    "            - self._cache[tuple(x)][\"edge_rotational_transform_over_n_field_periods\"]\n",
    "        )\n",
    "        return constraint_value\n",
    "\n",
    "    def enforce_constraints(self, x):\n",
    "        \"\"\"Enforce stellarator symmetry constraints.\"\"\"\n",
    "        # Reshape to 5x9 arrays\n",
    "        r_cos = x[:n_coefficients].reshape(self.n_poloidal_modes, self.n_toroidal_modes)\n",
    "        z_sin = x[n_coefficients : n_coefficients * 2].reshape(\n",
    "            self.n_poloidal_modes, self.n_toroidal_modes\n",
    "        )\n",
    "\n",
    "        # Enforce constraints for stellarator symmetry\n",
    "        # r_cos for m=0 and n<0 must be 0.0 for stellarator symmetric surfaces\n",
    "        r_cos[0, :4] = 0.0\n",
    "        # Major radius has to equal 1.0\n",
    "        r_cos[:, 4] = 1.0\n",
    "        # z_sin for m=0 and n<=0 must be 0.0 for stellarator symmetric surfaces\n",
    "        z_sin[0, :5] = 0.0\n",
    "\n",
    "        return torch.cat([r_cos.flatten(), z_sin.flatten()])\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the cache to free memory.\"\"\"\n",
    "        self._cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all the evaluation functions as a thin wrapper around methods of the `Evaluator` class to stick with semantics of the BoTorch example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(bounds=bounds)\n",
    "dim = evaluator.n_coeffecients * 2\n",
    "\n",
    "batch_size = 1\n",
    "max_cholesky_size = float(\"inf\")  # Always use Cholesky\n",
    "\n",
    "\n",
    "def eval_objective(x, evaluator):\n",
    "    \"\"\"Evaluate objective with proper normalization.\"\"\"\n",
    "    return evaluator.get_objective(x)\n",
    "\n",
    "\n",
    "def eval_c1(x, evaluator):\n",
    "    \"\"\"Evaluate constraint 1 with proper normalization.\"\"\"\n",
    "    return evaluator.aspect_ratio_constraint(x)\n",
    "\n",
    "\n",
    "def eval_c2(x, evaluator):\n",
    "    \"\"\"Evaluate constraint 2 with proper normalization.\"\"\"\n",
    "    return evaluator.average_triangularity_constraint(x)\n",
    "\n",
    "\n",
    "def eval_c3(x, evaluator):\n",
    "    \"\"\"Evaluate constraint 3 with proper normalization.\"\"\"\n",
    "    return evaluator.edge_rotational_transform_over_n_field_periods_constraint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the SCBO State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScboState:\n",
    "    dim: int\n",
    "    batch_size: int\n",
    "    length: float = 0.8\n",
    "    length_min: float = 0.5**7\n",
    "    length_max: float = 1.6\n",
    "    failure_counter: int = 0\n",
    "    failure_tolerance: int = float(\"nan\")  # Note: Post-initialized\n",
    "    success_counter: int = 0\n",
    "    success_tolerance: int = 10  # Note: The original paper uses 3\n",
    "    best_value: float = -float(\"inf\")\n",
    "    best_constraint_values: Tensor = torch.ones(2, **tkwargs) * torch.inf\n",
    "    restart_triggered: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.failure_tolerance = math.ceil(\n",
    "            max([4.0 / self.batch_size, float(self.dim) / self.batch_size])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to update the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScboState(dim=90, batch_size=1, length=0.8, length_min=0.0078125, length_max=1.6, failure_counter=0, failure_tolerance=90, success_counter=0, success_tolerance=10, best_value=-inf, best_constraint_values=tensor([inf, inf], dtype=torch.float64), restart_triggered=False)\n"
     ]
    }
   ],
   "source": [
    "def update_tr_length(state: ScboState):\n",
    "    # Update the length of the trust region according to\n",
    "    # success and failure counters\n",
    "    # (Just as in original TuRBO paper)\n",
    "    if state.success_counter == state.success_tolerance:  # Expand trust region\n",
    "        state.length = min(2.0 * state.length, state.length_max)\n",
    "        state.success_counter = 0\n",
    "    elif state.failure_counter == state.failure_tolerance:  # Shrink trust region\n",
    "        state.length /= 2.0\n",
    "        state.failure_counter = 0\n",
    "\n",
    "    if state.length < state.length_min:  # Restart when trust region becomes too small\n",
    "        state.restart_triggered = True\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def get_best_index_for_batch(Y: Tensor, C: Tensor):\n",
    "    \"\"\"Return the index for the best point.\"\"\"\n",
    "    is_feas = (C <= 0).all(dim=-1)\n",
    "    if is_feas.any():  # Choose best feasible candidate\n",
    "        score = Y.clone()\n",
    "        score[~is_feas] = -float(\"inf\")\n",
    "        return score.argmax()\n",
    "    return C.clamp(min=0).sum(dim=-1).argmin()\n",
    "\n",
    "\n",
    "def update_state(state, Y_next, C_next):\n",
    "    \"\"\"Method used to update the TuRBO state after each step of optimization.\n",
    "\n",
    "    Success and failure counters are updated according to the objective values\n",
    "    (Y_next) and constraint values (C_next) of the batch of candidate points\n",
    "    evaluated on the optimization step.\n",
    "\n",
    "    As in the original TuRBO paper, a success is counted whenver any one of the\n",
    "    new candidate points improves upon the incumbent best point. The key difference\n",
    "    for SCBO is that we only compare points by their objective values when both points\n",
    "    are valid (meet all constraints). If exactly one of the two points being compared\n",
    "    violates a constraint, the other valid point is automatically considered to be better.\n",
    "    If both points violate some constraints, we compare them inated by their constraint values.\n",
    "    The better point in this case is the one with minimum total constraint violation\n",
    "    (the minimum sum of constraint values)\"\"\"\n",
    "\n",
    "    # Pick the best point from the batch\n",
    "    best_ind = get_best_index_for_batch(Y=Y_next, C=C_next)\n",
    "    y_next, c_next = Y_next[best_ind], C_next[best_ind]\n",
    "\n",
    "    if (c_next <= 0).all():\n",
    "        # At least one new candidate is feasible\n",
    "        improvement_threshold = state.best_value + 1e-3 * math.fabs(state.best_value)\n",
    "        if y_next > improvement_threshold or (state.best_constraint_values > 0).any():\n",
    "            state.success_counter += 1\n",
    "            state.failure_counter = 0\n",
    "            state.best_value = y_next.item()\n",
    "            state.best_constraint_values = c_next\n",
    "        else:\n",
    "            state.success_counter = 0\n",
    "            state.failure_counter += 1\n",
    "    else:\n",
    "        # No new candidate is feasible\n",
    "        total_violation_next = c_next.clamp(min=0).sum(dim=-1)\n",
    "        total_violation_center = state.best_constraint_values.clamp(min=0).sum(dim=-1)\n",
    "        if total_violation_next < total_violation_center:\n",
    "            state.success_counter += 1\n",
    "            state.failure_counter = 0\n",
    "            state.best_value = y_next.item()\n",
    "            state.best_constraint_values = c_next\n",
    "        else:\n",
    "            state.success_counter = 0\n",
    "            state.failure_counter += 1\n",
    "\n",
    "    # Update the length of the trust region according to the success and failure counters\n",
    "    state = update_tr_length(state)\n",
    "    return state\n",
    "\n",
    "\n",
    "# Define example state\n",
    "state = ScboState(dim=dim, batch_size=batch_size)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function to generate a batch of candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(\n",
    "    state,\n",
    "    model,  # GP model\n",
    "    X,  # Evaluated points on the domain [0, 1]^d\n",
    "    Y,  # Function values\n",
    "    C,  # Constraint values\n",
    "    batch_size,\n",
    "    n_candidates,  # Number of candidates for Thompson sampling\n",
    "    constraint_model,\n",
    "    sobol: SobolEngine,\n",
    "):\n",
    "    # assert X.min() >= 0.0 and X.max() <= 1.0 and torch.all(torch.isfinite(Y))\n",
    "\n",
    "    # Create the TR bounds\n",
    "    best_ind = get_best_index_for_batch(Y=Y, C=C)\n",
    "    x_center = X[best_ind, :].clone()\n",
    "    tr_lb = torch.clamp(x_center - state.length / 2.0, 0.0, 1.0)\n",
    "    tr_ub = torch.clamp(x_center + state.length / 2.0, 0.0, 1.0)\n",
    "\n",
    "    # Thompson Sampling w/ Constraints (SCBO)\n",
    "    dim = X.shape[-1]\n",
    "    pert = sobol.draw(n_candidates).to(dtype=torch.float32, device=device)\n",
    "    pert = tr_lb + (tr_ub - tr_lb) * pert\n",
    "\n",
    "    # Create a perturbation mask\n",
    "    prob_perturb = min(20.0 / dim, 1.0)\n",
    "    mask = torch.rand(n_candidates, dim, **tkwargs) <= prob_perturb\n",
    "    ind = torch.where(mask.sum(dim=1) == 0)[0]\n",
    "    mask[ind, torch.randint(0, dim - 1, size=(len(ind),), device=device)] = 1\n",
    "\n",
    "    # Create candidate points from the perturbations and the mask\n",
    "    X_cand = x_center.expand(n_candidates, dim).clone()\n",
    "    X_cand[mask] = pert[mask]\n",
    "\n",
    "    # Sample on the candidate points using Constrained Max Posterior Sampling\n",
    "    constrained_thompson_sampling = ConstrainedMaxPosteriorSampling(\n",
    "        model=model, constraint_model=constraint_model, replacement=False\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        X_next = constrained_thompson_sampling(X_cand, num_samples=batch_size)\n",
    "\n",
    "    return X_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve an initial dataset to train the GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_init = 100\n",
    "\n",
    "# Generate initial data\n",
    "train_X, y_init, c1_init, c2_init, c3_init = (\n",
    "    filtered_tensor[:n_init, :90],\n",
    "    filtered_tensor[:n_init, 90],\n",
    "    filtered_tensor[:n_init, 91],\n",
    "    filtered_tensor[:n_init, 92],\n",
    "    filtered_tensor[:n_init, 93],\n",
    ")\n",
    "train_Y = y_init.unsqueeze(-1)\n",
    "C1 = c1_init.unsqueeze(-1)\n",
    "C2 = c2_init.unsqueeze(-1)\n",
    "C3 = c3_init.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the SCBO algorithm (looks very much like TuRBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "Training Model\n",
      "Training Model\n",
      "Training Model\n",
      "Started TuRBO\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Thread 0:\n\tFATAL ERROR in thread=0Thread 0:\n\tFATAL ERROR in thread=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 62\u001b[0m\n\u001b[1;32m     46\u001b[0m     X_next \u001b[38;5;241m=\u001b[39m generate_batch(\n\u001b[1;32m     47\u001b[0m         state\u001b[38;5;241m=\u001b[39mstate,\n\u001b[1;32m     48\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m         sobol\u001b[38;5;241m=\u001b[39msobol,\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Evaluate both the objective and constraints for the selected candidaates\u001b[39;00m\n\u001b[1;32m     61\u001b[0m Y_next \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m---> 62\u001b[0m     [eval_objective(x, evaluator) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_next], dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     63\u001b[0m )\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     64\u001b[0m C1_next \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     65\u001b[0m     [eval_c1(x, evaluator) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_next], dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     66\u001b[0m )\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     67\u001b[0m C2_next \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     68\u001b[0m     [eval_c2(x, evaluator) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_next], dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     69\u001b[0m )\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 62\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m     X_next \u001b[38;5;241m=\u001b[39m generate_batch(\n\u001b[1;32m     47\u001b[0m         state\u001b[38;5;241m=\u001b[39mstate,\n\u001b[1;32m     48\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m         sobol\u001b[38;5;241m=\u001b[39msobol,\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Evaluate both the objective and constraints for the selected candidaates\u001b[39;00m\n\u001b[1;32m     61\u001b[0m Y_next \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m---> 62\u001b[0m     [\u001b[43meval_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_next], dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     63\u001b[0m )\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     64\u001b[0m C1_next \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     65\u001b[0m     [eval_c1(x, evaluator) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_next], dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     66\u001b[0m )\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     67\u001b[0m C2_next \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     68\u001b[0m     [eval_c2(x, evaluator) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_next], dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     69\u001b[0m )\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36meval_objective\u001b[0;34m(x, evaluator)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21meval_objective\u001b[39m(x, evaluator):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate objective with proper normalization.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m, in \u001b[0;36mEvaluator.get_objective\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_objective\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 55\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;28mtuple\u001b[39m(x)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_elongation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m     boundary_surface \u001b[38;5;241m=\u001b[39m SurfaceRZFourier(\n\u001b[1;32m     40\u001b[0m         r_cos\u001b[38;5;241m=\u001b[39mr_cos\u001b[38;5;241m.\u001b[39mnumpy(), z_sin\u001b[38;5;241m=\u001b[39mz_sin\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Evaluate the forward model with the boundary surface and retrieve results\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     metrics, _ \u001b[38;5;241m=\u001b[39m \u001b[43mforward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboundary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboundary_surface\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[x_tuple] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_elongation\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics\u001b[38;5;241m.\u001b[39mmax_elongation,\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maspect_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics\u001b[38;5;241m.\u001b[39maspect_ratio,\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage_triangularity\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics\u001b[38;5;241m.\u001b[39maverage_triangularity,\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_rotational_transform_over_n_field_periods\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics\u001b[38;5;241m.\u001b[39medge_rotational_transform_over_n_field_periods,\n\u001b[1;32m     50\u001b[0m     }\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[x_tuple]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/hf-stellarator-NEffnWGp-py3.10/lib/python3.10/site-packages/constellaration/forward_model.py:106\u001b[0m, in \u001b[0;36mforward_model\u001b[0;34m(boundary, ideal_mhd_parameters, settings)\u001b[0m\n\u001b[1;32m    101\u001b[0m     settings \u001b[38;5;241m=\u001b[39m ConstellarationSettings()\n\u001b[1;32m    102\u001b[0m vmec_settings \u001b[38;5;241m=\u001b[39m vmec_settings_module\u001b[38;5;241m.\u001b[39mcreate_vmec_settings_from_preset(\n\u001b[1;32m    103\u001b[0m     boundary,\n\u001b[1;32m    104\u001b[0m     settings\u001b[38;5;241m=\u001b[39msettings\u001b[38;5;241m.\u001b[39mvmec_preset_settings,\n\u001b[1;32m    105\u001b[0m )\n\u001b[0;32m--> 106\u001b[0m equilibrium \u001b[38;5;241m=\u001b[39m \u001b[43mvmec_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_vmec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboundary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboundary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmhd_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mideal_mhd_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvmec_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmec_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Geometrical metrics\u001b[39;00m\n\u001b[1;32m    113\u001b[0m (\n\u001b[1;32m    114\u001b[0m     n_poloidal_points,\n\u001b[1;32m    115\u001b[0m     n_toroidal_points,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     max_toroidal_mode\u001b[38;5;241m=\u001b[39mequilibrium\u001b[38;5;241m.\u001b[39mntor,\n\u001b[1;32m    119\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/hf-stellarator-NEffnWGp-py3.10/lib/python3.10/site-packages/constellaration/mhd/vmec_utils.py:224\u001b[0m, in \u001b[0;36mrun_vmec\u001b[0;34m(boundary, mhd_parameters, vmec_settings)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run VMEC++ in fixed-boundary mode.\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m vmec_indata \u001b[38;5;241m=\u001b[39m build_vmecpp_indata(\n\u001b[1;32m    219\u001b[0m     mhd_parameters\u001b[38;5;241m=\u001b[39mmhd_parameters,\n\u001b[1;32m    220\u001b[0m     boundary\u001b[38;5;241m=\u001b[39mboundary,\n\u001b[1;32m    221\u001b[0m     vmec_settings\u001b[38;5;241m=\u001b[39mvmec_settings,\n\u001b[1;32m    222\u001b[0m )\n\u001b[0;32m--> 224\u001b[0m output_quantities \u001b[38;5;241m=\u001b[39m \u001b[43mvmecpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvmec_indata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmec_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmec_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vmecppwout_from_wout(output_quantities\u001b[38;5;241m.\u001b[39mwout)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/hf-stellarator-NEffnWGp-py3.10/lib/python3.10/site-packages/vmecpp/__init__.py:1230\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, magnetic_field, max_threads, verbose, restart_from)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magnetic_field \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1230\u001b[0m     cpp_output_quantities \u001b[38;5;241m=\u001b[39m \u001b[43m_vmecpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpp_indata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;66;03m# magnetic_response_table takes precedence anyway, but let's be explicit, to ensure\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m     \u001b[38;5;66;03m# we don't silently use the mgrid file in input, instead of the magnetic_response_table object.\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m     cpp_indata\u001b[38;5;241m.\u001b[39mmgrid_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNONE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Thread 0:\n\tFATAL ERROR in thread=0Thread 0:\n\tFATAL ERROR in thread=0"
     ]
    }
   ],
   "source": [
    "# Initialize TuRBO state\n",
    "state = ScboState(dim, batch_size=batch_size)\n",
    "\n",
    "# Note: We use 2000 candidates here to make the tutorial run faster.\n",
    "# SCBO actually uses min(5000, max(2000, 200 * dim)) candidate points by default.\n",
    "N_CANDIDATES = 2000 if not SMOKE_TEST else 4\n",
    "sobol = SobolEngine(dim, scramble=True, seed=1)\n",
    "\n",
    "\n",
    "def get_fitted_model(X, Y):\n",
    "    likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "    covar_module = ScaleKernel(  # Use the same lengthscale prior as in the TuRBO paper\n",
    "        MaternKernel(\n",
    "            nu=2.5, ard_num_dims=dim, lengthscale_constraint=Interval(0.005, 4.0)\n",
    "        )\n",
    "    )\n",
    "    model = SingleTaskGP(\n",
    "        X,\n",
    "        Y,\n",
    "        covar_module=covar_module,\n",
    "        likelihood=likelihood,\n",
    "        # input_transform=Normalize(d=X.shape[-1]),\n",
    "        outcome_transform=Standardize(m=1),\n",
    "    )\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "    print(\"Training Model\")\n",
    "\n",
    "    with gpytorch.settings.max_cholesky_size(max_cholesky_size):\n",
    "        fit_gpytorch_mll(mll)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "while not state.restart_triggered:  # Run until TuRBO converges\n",
    "    # Fit GP models for objective and constraints\n",
    "    model = get_fitted_model(train_X, train_Y)\n",
    "    c1_model = get_fitted_model(train_X, C1)\n",
    "    c2_model = get_fitted_model(train_X, C2)\n",
    "    c3_model = get_fitted_model(train_X, C3)\n",
    "\n",
    "    print(f\"Started TuRBO\")\n",
    "\n",
    "    # Generate a batch of candidates\n",
    "    with gpytorch.settings.max_cholesky_size(max_cholesky_size):\n",
    "        X_next = generate_batch(\n",
    "            state=state,\n",
    "            model=model,\n",
    "            # X=train_X,\n",
    "            # Y=train_Y,\n",
    "            X=model.train_inputs[0],\n",
    "            Y=model.train_targets,\n",
    "            C=torch.cat((C1, C2, C3), dim=-1),\n",
    "            batch_size=batch_size,\n",
    "            n_candidates=N_CANDIDATES,\n",
    "            constraint_model=ModelListGP(c1_model, c2_model, c3_model),\n",
    "            sobol=sobol,\n",
    "        )\n",
    "\n",
    "    # Evaluate both the objective and constraints for the selected candidaates\n",
    "    Y_next = torch.tensor(\n",
    "        [eval_objective(x, evaluator) for x in X_next], dtype=dtype, device=device\n",
    "    ).unsqueeze(-1)\n",
    "    C1_next = torch.tensor(\n",
    "        [eval_c1(x, evaluator) for x in X_next], dtype=dtype, device=device\n",
    "    ).unsqueeze(-1)\n",
    "    C2_next = torch.tensor(\n",
    "        [eval_c2(x, evaluator) for x in X_next], dtype=dtype, device=device\n",
    "    ).unsqueeze(-1)\n",
    "    C3_next = torch.tensor(\n",
    "        [eval_c3(x, evaluator) for x in X_next], dtype=dtype, device=device\n",
    "    ).unsqueeze(-1)\n",
    "    C_next = torch.cat([C1_next, C2_next, C3_next], dim=-1)\n",
    "\n",
    "    # Update TuRBO state\n",
    "    state = update_state(state=state, Y_next=Y_next, C_next=C_next)\n",
    "\n",
    "    # Append data. Note that we append all data, even points that violate\n",
    "    # the constraints. This is so our constraint models can learn more\n",
    "    # about the constraint functions and gain confidence in where violations occur.\n",
    "    train_X = torch.cat((train_X, X_next), dim=0)\n",
    "    train_Y = torch.cat((train_Y, Y_next), dim=0)\n",
    "    C1 = torch.cat((C1, C1_next), dim=0)\n",
    "    C2 = torch.cat((C2, C2_next), dim=0)\n",
    "    C3 = torch.cat((C3, C3_next), dim=0)\n",
    "\n",
    "    # Print current status. Note that state.best_value is always the best\n",
    "    # objective value found so far which meets the constraints, or in the case\n",
    "    # that no points have been found yet which meet the constraints, it is the\n",
    "    # objective value of the point with the minimum constraint violation.\n",
    "    if (state.best_constraint_values <= 0).all():\n",
    "        print(\n",
    "            f\"{len(train_X)}) Best value: {state.best_value:.2e}, TR length: {state.length:.2e}\"\n",
    "        )\n",
    "    else:\n",
    "        violation = state.best_constraint_values.clamp(min=0).sum()\n",
    "        print(\n",
    "            f\"{len(train_X)}) No feasible point yet! Smallest total violation: \"\n",
    "            f\"{violation:.2e}, TR length: {state.length:.2e}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-stellarator-NEffnWGp-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
